{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3BoBydmNdBn"
   },
   "source": [
    "# <font>Data Science</font>\n",
    "\n",
    "## <font>Fundamentos de Linguagem Python Para Análise de Dados e Data Science</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-XiUxuQxNdBo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.11.5\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XM0o8ivQCJA1"
   },
   "source": [
    "## Projeto - Modelo de Linguagem\n",
    "\n",
    "Os  modelos  de  linguagem  são  tipos  de  modelos  de  aprendizado  de  máquina (Machine Learning) que  foram  treinados  para  entender  e  gerar  texto  em  linguagem  humana.  Eles  são capazes de realizar várias tarefas relacionadas à linguagem, como tradução de texto, resposta a perguntas, resumo de texto, geração de texto criativo e muito mais.\n",
    "\n",
    "Os modelos de linguagem são treinados em grandes quantidades de texto para aprender padrões  na  linguagem  humana,  incluindo  gramática,  sintaxe,  estilo,  tom  e  até  mesmo  alguns fatossobre  o  mundo.  Uma  vez  treinados,  eles  podem  gerar  texto  que  é  frequentemente indistinguível do texto escrito por humanos.\n",
    "\n",
    "Existem vários tipos de modelos de linguagem, mas os mais recentes e mais poderosos são baseados em uma técnica chamada Transformer. Esses modelos, como o GPT-4 da OpenAI, usam   uma   arquitetura   de   rede   neural artificial conhecida   como   Transformer,   que   é especialmente  eficaz  para  entender  o  contexto  e  a  estrutura  em  sequências  de  texto\n",
    "\n",
    "### Generative Pre-Trained Transformer (GPT)\n",
    "\n",
    "GPT significa Generative Pre-Trained Transformer, um modelo lançado pela OpenAI em 2018.  É  um  modelo  de  linguagem  desenvolvido  para  obter  texto  como  se  fosse  gerado  por humanos.\n",
    "\n",
    "Baseia-se  principalmente  no  conceito  de  transformadores,  que  forma  a  base  de  seu algoritmo. Transformer é um tipo de arquitetura de rede neural que usa uma camada de auto-atenção para identificar as relações entre diferentes partes da entrada, como palavras em uma frase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHOz2XsxCtL0"
   },
   "source": [
    "### Construindo ChatbotPersonalizado com GPT-4 e Linguagem Python\n",
    "\n",
    "Documentação: https://pypi.org/project/openai/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Nh3miE18CJFc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==0.28 in c:\\users\\informatica\\anaconda3\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\informatica\\anaconda3\\lib\\site-packages (from openai==0.28) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\informatica\\anaconda3\\lib\\site-packages (from openai==0.28) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\informatica\\anaconda3\\lib\\site-packages (from openai==0.28) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\informatica\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\informatica\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\informatica\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\informatica\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\informatica\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\informatica\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\informatica\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\informatica\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\informatica\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\informatica\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\informatica\\anaconda3\\lib\\site-packages (from tqdm->openai==0.28) (0.4.6)\n",
      "Requirement already satisfied: requests in c:\\users\\informatica\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\informatica\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\informatica\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\informatica\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\informatica\\anaconda3\\lib\\site-packages (from requests) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==0.28\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZiYkYOG7C7g8"
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import openai\n",
    "import requests\n",
    "\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cHjC-7JsC7jT"
   },
   "outputs": [],
   "source": [
    "# Função para gerar texto a partir do modelo de linguagem\n",
    "def gera_texto(texto):\n",
    "    # A função openai.Completion.create é utilizada para fazer uma solicitação ao modelo GPT da OpenAI\n",
    "    response = openai.Completion.create(\n",
    "        # Modelo utilizado\n",
    "        engine='gpt-4o-mini',  # Especifica o modelo GPT que será utilizado. (Neste caso, parece ser um erro de digitação, o nome correto seria 'gpt-4' ou outro modelo disponível na plataforma).\n",
    "\n",
    "        # Texto inicial da conversa com o chatbot\n",
    "        prompt=texto,  # O parâmetro 'prompt' é o texto que será passado ao modelo como entrada. Ele pode ser uma pergunta, um contexto ou qualquer outro tipo de informação inicial. O modelo irá gerar a resposta com base nesse texto.\n",
    "\n",
    "        # Comprimento da resposta gerada pelo modelo\n",
    "        max_tokens=150,  # Define o número máximo de \"tokens\" (palavras ou pedaços de palavras) que a resposta gerada pode ter. Aqui, o máximo é 150 tokens, o que geralmente corresponde a uma resposta de tamanho médio.\n",
    "\n",
    "        # Número de conclusões gerar para cada prompt\n",
    "        n=1,  # Define quantas respostas diferentes o modelo deve gerar. No caso, está sendo solicitado apenas uma resposta.\n",
    "\n",
    "        # O texto gerado não conterá a sequência de parada\n",
    "        stop=None,  # O parâmetro 'stop' permite definir uma sequência de parada que, se encontrada no texto gerado, fará com que o modelo pare de gerar mais palavras. Aqui, está configurado como 'None', o que significa que não há sequência específica para parar a geração.\n",
    "\n",
    "        # Valores próximos de 1 significam que a saída é mais aleatória\n",
    "        temperature=0.5,  # O parâmetro 'temperature' controla a aleatoriedade da resposta gerada. Valores próximos de 0 resultam em respostas mais determinísticas (previsíveis), enquanto valores mais altos (perto de 1) fazem com que as respostas sejam mais variadas e criativas. Aqui, 0.5 é um equilíbrio entre previsibilidade e aleatoriedade.\n",
    "    )\n",
    "\n",
    "    # A função retorna o texto gerado pela primeira (e única) conclusão do modelo.\n",
    "    return response.choices[0].text.strip()  # 'choices[0]' seleciona a primeira resposta do modelo. 'strip()' remove espaços em branco extras no início e no final da resposta gerada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIOZXvAtC7ld",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bem-vindo ao Chatbot personalizado!\n",
      "Digite 'sair' a qualquer momento para encerrar a conversa.\n",
      "Você: olá\n",
      "Chatbot: Olá! Como posso ajudar você hoje?\n"
     ]
    }
   ],
   "source": [
    "# Função para gerar o texto (exemplo básico)\n",
    "def gera_texto(gpt_prompt):\n",
    "    # Para fins de exemplo, vamos apenas retornar uma resposta simples com base no prompt.\n",
    "    # Aqui você pode integrar com a API do OpenAI para obter uma resposta real.\n",
    "    if \"olá\" in gpt_prompt.lower():\n",
    "        return \"Olá! Como posso ajudar você hoje?\"\n",
    "    elif \"tudo bem\" in gpt_prompt.lower():\n",
    "        return \"Estou bem, obrigado por perguntar! E você?\"\n",
    "    else:\n",
    "        return \"Desculpe, não entendi. Pode reformular?\"\n",
    "    \n",
    "# Função principal do programa\n",
    "def main():\n",
    "    print(\"Bem-vindo ao Chatbot personalizado da loja Essenza!\")\n",
    "    print(\"Digite 'sair' a qualquer momento para encerrar a conversa.\")\n",
    "\n",
    "    # Loop\n",
    "    while True:\n",
    "        # Coleta a pergunta digitada pelo usuário.\n",
    "        user_input = input(\"Você: \")\n",
    "\n",
    "        # Se a mensagem for 'sair', finaliza o programa.\n",
    "        if user_input.lower() == \"sair\":\n",
    "            print(\"Chatbot: Até a próxima!\")\n",
    "            break\n",
    "\n",
    "        # Coloca a mensagem digitada pelo usuário em uma variável\n",
    "        gpt_prompt = f\"\\nUsuário: {user_input}\\nChatbot:\"\n",
    "\n",
    "        # Obtém a resposta do modelo executando a função gera_texto()\n",
    "        chatbot_response = gera_texto(gpt_prompt)\n",
    "\n",
    "        # Imprime a resposta do chatbot\n",
    "        print(\"Chatbot:\", chatbot_response)\n",
    "\n",
    "# Execução do programa\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhqYTf2FHf3j"
   },
   "source": [
    "## Exercício\n",
    "\n",
    "Você vai criar um chatbot para atender clientes de uma loja online. O chatbot deve ser capaz de:\n",
    "\n",
    "1. Saudar o cliente e perguntar como pode ajudar.\n",
    "2. Responder perguntas sobre produtos, como preço e descrição.\n",
    "3. Simular o processo de compra de um produto.\n",
    "4. Oferecer ajuda em caso de problemas, como problemas de pagamento.\n",
    "5. Encerrar a conversa de forma amigável quando o usuário digitar \"sair\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMS6j4cYLdAk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DT3aufY7mCNA"
   },
   "source": [
    "## Fim"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "WO_4W2dlXqM0",
    "rS-UwlbAZMWU",
    "yL0zHXe-ZpMs",
    "ATE6Nys9ZyUG",
    "wCQSTvp1bvvd",
    "AXG2nZ2TcH7N",
    "ynKy1MmocivN",
    "wqvixxFJc3Ce",
    "GvHOj97Ldglp",
    "ebtAUPFxdqv1",
    "ddEubRVmehku",
    "yNdz5HNHexju",
    "KYTMVg2HfYPW",
    "sO_Asyd_fpjn"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
